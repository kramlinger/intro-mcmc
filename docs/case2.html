<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Case Study II: Hybrid Gibbs and Credible Interval Estimation</title>

<script src="site_libs/header-attrs-2.5/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="metropolis-hastings.html">Intro to Metropolis-Hastings</a>
</li>
<li>
  <a href="gibbs-intro.html">Motivation for Gibbs</a>
</li>
<li>
  <a href="credible_intervals.html">Credible Intervals</a>
</li>
<li>
  <a href="case1.html">Case Study I</a>
</li>
<li>
  <a href="case2.html">Case Study II</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Case Study II: Hybrid Gibbs and Credible Interval Estimation</h1>

</div>


<hr />
<p>This document gives an example on the implementation of a hybrid Gibbs Sampler and a variety of MCMC based credible interval estimators. It is based on data gathered from <a href="https://projects.fivethirtyeight.com/polls/florida/">FiveThirtyEight</a> and made available <a href="https://github.com/kramlinger/intro-mcmc/blob/main/florida.csv">here</a>.<br />
The problem is motivated by Scott M. Lynch (2007): Introduction to Applied Bayesian Statistics and Estimation for Social Scientists, Chapter 9.</p>
<hr />
<div id="the-data-model" class="section level4">
<h4>The Data &amp; Model</h4>
<p>In the 2020 presidential election, the state of Florida was considered a swing state. Due to is large population, it contributed 29 electoral votes to the electoral college and was therefore fiercely contested by both campaigns. The following table shows the results of selected polls and the general election.</p>
<pre class="r"><code>df = read.csv(&quot;florida.csv&quot;)
df</code></pre>
<pre><code>##                         SOURCE        DATE   BIDEN   TRUMP
## 1            Insider Advantage  2020-11-02     188     192
## 2        Quinnipiac University  2020-11-01     779     696
## 3                        Ipsos  2020-11-01     335     308
## 4 Redfield &amp; Wilton Strategies  2020-10-28     773     601
## 5             General Election  2020-11-02 5295138 5667716</code></pre>
<p>Consider the following simplifying model. For each poll <span class="math inline">\(i = 1, \dots 4\)</span>, let the total number of likely voters <span class="math inline">\(n_i\in\mathbb{N}\)</span> be known, <span class="math inline">\(X_i\in\mathbb{N}\)</span> the number of votes for Biden and <span class="math inline">\(p_i\in (0,1)\)</span> be the probability that an individual voter casts his vote for Biden. Assuming a binomial likelihood <span class="math inline">\(X_i | p_i \sim Bin(n_i, p_i)\)</span> for each poll and independent surveys, this gives <span class="math display">\[\begin{equation*}
  P(X = x| p ) \propto \prod_{i=1}^4 p_i^{x_i} (1-p_i)^{n_i-x_i}
\end{equation*}\]</span> Furthermore, let the prior be given by the product of <span class="math inline">\(Beta(a,b)\)</span>-densities, so that <span class="math display">\[\begin{equation*}
  \pi(p | a,b ) \propto \prod_{i=1}^4 \frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\; p_i^{a-1}(1-p_i)^{b-1}.
\end{equation*}\]</span> Let us derive the full conditionals. The above specification gives the density <span class="math display">\[\begin{align*}
  \pi(p, a, b| x)
  &amp;\propto f(x|p)\pi(p|a,b)\pi(a,b)
  \propto \left(\frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\right)^4 \Big( p_i^{x_i+a-1}(1-p_i)^{n_i - x_i + b-1} \Big)\pi(a,b).
\end{align*}\]</span> The full conditionals for <span class="math inline">\(p_i\)</span>, <span class="math inline">\(i=1, \dots, 4\)</span> are thus given by <span class="math display">\[\begin{equation*}
  \pi(p_i| x_i, a, b) \propto p_i^{x_i+a-1} (1-p_i)^{n_i-x_i+b-1}.
\end{equation*}\]</span> For the hyperparameters, <span class="math display">\[\begin{align*}
\pi(a | x, p, b)
&amp;\propto
\left(\frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\right)^4
\exp\left\{
  a\sum_{i=1}^4 \ln(p_i)
\right\}\pi(a,b),\\
\pi(b | x, p, a)
&amp;\propto
\left(\frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\right)^4
\exp\left\{
  b\sum_{i=1}^4 \ln(1-p_i)
\right\}\pi(a,b).
\end{align*}\]</span> For the distributional characterization of the hyperparameters, it is required to ensure both propriety and positivity of <span class="math inline">\(a,b&gt;0\)</span>. For <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> independently Gamma-distributed, <span class="math display">\[\begin{align*}
  \pi(a) &amp;\propto a^{\alpha -1 }\exp(-\beta a),\\
  \pi(b) &amp;\propto b^{\gamma -1 }\exp(-\delta b).
\end{align*}\]</span> This gives the full conditionals <span class="math display">\[\begin{align*}
\pi(a | x, p, b)
&amp;\propto
\left(\frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\right)^4
a^{\alpha-1}
\exp\left[
  a\left\{ -\beta + \sum_{i=1}^4 \ln(p_i) \right\}
\right],\\
\pi(b | x, p, a)
&amp;\propto
\left(\frac{\Gamma(a+b)}{\Gamma(a) + \Gamma(b)}\right)^4
b^{\gamma-1}
\exp\left[
  b\left\{ -\delta + \sum_{i=1}^4 \ln(1-p_i) \right\}
\right].
\end{align*}\]</span> Since it is reasonable to expect that <span class="math inline">\(p_i\)</span> is close to <span class="math inline">\(0.5\)</span>, we may set <span class="math inline">\(\alpha = \gamma = 6.25\)</span> and <span class="math inline">\(\beta = \delta = 0.025\)</span>.</p>
<p>Eventually, the hierarchy is completely specified with Beta and Gamma priors. We have obtained all full conditionals.</p>
<p>However, it is not clear how to draw samples from the full conditionals for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. This leads to the <em>hybrid Gibbs Sampler</em>. In each step in which it is to be sampled from <span class="math inline">\(\pi(a|x,p,b)\)</span> and <span class="math inline">\(\pi(b|x,p,a)\)</span>, a random walk Metropolis-Hastings algorithm is run. For both Markov chains <span class="math inline">\((a_n)\)</span> and <span class="math inline">\((b_n)\)</span>, we can take a Gaussian proposal which is sufficiently spread out, e.g. <span class="math inline">\(a_{n+1} \sim \mathcal{N}(a_n, 20)\)</span> for <span class="math inline">\((a_n)\)</span>.</p>
<hr />
</div>
<div id="the-hybrid-gibbs-sampler" class="section level4">
<h4>The Hybrid Gibbs Sampler</h4>
<p>We implement the algorithm as outlined above. It takes the votes for Biden (<code>x</code>) and total votes (<code>n</code>) as arguments.</p>
<p>Note that the conditional(s) <code>ln.pi</code> is implemented in log-transformed form for computational purposes. The full conditionals for <span class="math inline">\(\pi(a|x,p,b)\)</span> and <span class="math inline">\(\pi(b|x,p,a)\)</span> coincide if the arguments are adapted, which is why the single function <code>ln.pi</code> is enough.</p>
<p>The Metropolis-Hastings step is straight-forward: First, a proposal is generated. If it is negative, <code>rho</code> would be zero. Since we implemented only the log-transformed version <code>ln.rho</code>, such proposals are immediately rejected without generating a uniform random variable. If the proposal is positive (the <code>else</code>-statement), the acceptance probability is calculated and evaluated on a uniformly generated random variable.</p>
<pre class="r"><code>HybridGibbs &lt;- function(x, n){
  np &lt;- length(x) #number of polls
  
  ### full conditionals 
  ln.pi &lt;- function(a, b, p) { # full conditional for both a and b, use &quot;p&quot; and &quot;1-p&quot; 
    -np * lbeta(a, b) + log(a) * (6.25 - 1) + a * (-0.025 + sum(log(p)))
  }
  
  ### set up
  N &lt;- 1e5 #length of chain
  P &lt;- matrix(NA, nrow = N, ncol = np) #states of Markov chain for p  
  a &lt;- b &lt;- rep(NA, N) #states of Markov chain for a and b
  
  ### initial values
  P[1, ] &lt;- x / n
  b[1] &lt;- a[1] &lt;- 10
  
  for (i in 2:N) {
    
    ## MH for a
    y &lt;- rnorm(1, a[i - 1], 20)
    if (y &lt; 0) a[i] &lt;- a[i - 1] 
    else {
      ln.rho &lt;- ln.pi(y, b[i - 1], P[i - 1,]) - ln.pi(a[i - 1], b[i - 1], P[i - 1,])
      if (log(runif(1)) &lt; ln.rho) a[i] &lt;- y else a[i] &lt;- a[i - 1]
    }
    
    ## MH for b
    y &lt;- rnorm(1, b[i - 1], 20)
    if (y &lt; 0) b[i] &lt;- b[i - 1] 
    else {
      ln.rho &lt;- ln.pi(y, a[i], 1 - P[i - 1,]) - ln.pi(b[i - 1], a[i], 1 - P[i - 1,])
      if (log(runif(1)) &lt; ln.rho) b[i] &lt;- y else b[i] &lt;- b[i - 1]
    }
    
    ## full conditional for p
    P[i,] &lt;- rbeta(np, (x + a[i]),(n - x + b[i]))
  }
  
  return(list(P = P,a = a,b = b))
}</code></pre>
<p>Now, we can run the algorithm on the data. We print the Bayes estimator under squared error loss for each <span class="math inline">\(p_i\)</span>, <span class="math inline">\(i=1, \dots, 4\)</span>.</p>
<pre class="r"><code>x = df$BIDEN[-5]
n = x + df$TRUMP[-5]

Poll4 = HybridGibbs(x, n)
colMeans(Poll4$P) #Bayes estimator</code></pre>
<pre><code>## [1] 0.5145265 0.5287305 0.5250237 0.5542323</code></pre>
<p>In all four polls we observe that <span class="math inline">\(\widehat{p}_i &gt; 0.5\)</span>, even in the one from Insider Advantage. The list <code>Poll4</code> can be further investigated to check for convergence using graphical tools.</p>
<p>We continue to pool all polls into a single survey to proceed. It is obvious that only the inputs have to be aggregated, the model doesn’t change after setting <span class="math inline">\(i=1\)</span>.</p>
<pre class="r"><code>PollPool = HybridGibbs(sum(x), sum(n))
colMeans(PollPool$P) #Bayes estimator</code></pre>
<pre><code>## [1] 0.5358294</code></pre>
<p>Unsurprisingly, the pooled-poll Bayes estimator is not more informative than the separated poll estimators.</p>
<hr />
</div>
<div id="the-two-level-hierachy" class="section level4">
<h4>The Two-Level Hierachy</h4>
<p>We want to investigate if the additional flexibility gained with the third level of the hierarchy, the Gamma priors on <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, is contributing to a better estimation. In order to do so, we consider the two-level hierarchy and set</p>
<pre class="r"><code># set prior and likelihood parameters
alpha &lt;- 0.05
a &lt;- b &lt;- 250

# set posterior parameters
a.star &lt;- a + sum(x[1:4])
b.star &lt;- b + sum(n[1:4]) - sum(x[1:4])</code></pre>
<p>In the two-level hierarchy, the posterior is known. Therefore, we can directly compute the HPD credible interval.</p>
<pre class="r"><code># the HPD credible interval is the minimizer or
Q &lt;- function(arg, a0 = alpha, a = a.star, b = b.star) {
  abs(dbeta(arg[2], a, b) - dbeta(arg[1], a, b)) + 
    abs(pbeta(arg[2], a, b) - pbeta(arg[1], a, b) - (1 - a0))
}
CI0 &lt;- optim(qbeta(c(alpha/2, 1 - alpha/2), a.star, b.star), Q,
            lower = c(0,0), 
            upper = c(1,1), 
            method = &quot;L-BFGS-B&quot;)$par
CI0</code></pre>
<pre><code>## [1] 0.5169908 0.5465681</code></pre>
<hr />
</div>
<div id="credible-sets-for-the-three-level-hierarchy" class="section level4">
<h4>Credible Sets for the Three-Level-Hierarchy</h4>
<p>Now, we like to compare the derived analytic HPD for the two-level hierarchy with the MCMC based estimations in the three level hierarchy. First, we gather all the variables needed.</p>
<pre class="r"><code># gather params
p &lt;- PollPool$P #Markov chain of interest
a &lt;- PollPool$a #Markov chain of hyperparameters
b &lt;- PollPool$b #Markov chain of hyperparameters
a.post &lt;- a + sum(x) #posterior params
b.post &lt;- b + sum(n) - sum(x) #posterior params</code></pre>
<p>The simple MCMC based credible set estimators are taken from Eberly &amp; Casella. We consider the naive (<code>CI1</code>), order statistics based (<code>CI2</code>), CMDE (<code>CI3</code>) and weighted average (<code>CI4</code>) estimator.</p>
<pre class="r"><code># Naive
lb = qbeta(alpha/2, (x + a),(n - x + b))
ub = qbeta(1 - alpha/2, (x + a),(n - x + b))
CI1 = c(mean(lb), mean(ub))

# Order statistics estimator 
o.p = sort(p)
N = 1e5
CI2 = c(o.p[N * alpha / 2], o.p[N * (1 - alpha / 2)])

# CMDE
Q = function(arg) { 
  abs(mean(pbeta(arg[2], a.post, b.post) - pbeta(arg[1], a.post, b.post)) - 
    (1 - alpha))
}
CI3 = optim(CI2, Q,
            lower = c(0,0), 
            upper = c(1,1), 
            method = &quot;L-BFGS-B&quot;)$par

# weighted average construction
lbw = mean(lb * dbeta( lb , a.post, b.post)) / mean(dbeta( lb , a.post, b.post))
ubw = mean(ub * dbeta( ub , a.post, b.post)) / mean(dbeta( ub , a.post, b.post))
CI4 = c(lbw, ubw)


require(&#39;tidyverse&#39;)
tbl0 = rbind(CI1, CI2, CI3, CI4) # variants of CI
tbl0 = cbind(tbl0, apply(tbl0, 1, diff)) # add length
rownames(tbl0) = c(&quot;Naive&quot;, &quot;Order&quot;, &quot;CMDE&quot;, &quot;Weight&quot;)
colnames(tbl0) = c(&quot;lower&quot;, &quot;upper&quot;, &quot;diff&quot;)
tbl0</code></pre>
<pre><code>##            lower     upper        diff
## Naive  0.5056176 0.5590691 0.053451470
## Order  0.5201440 0.5515138 0.031369850
## CMDE   0.5201440 0.5515138 0.031369850
## Weight 0.5336654 0.5414075 0.007742125</code></pre>
<p>We know that the first credible set cannot be trusted. Trustable, and shortest, is the weighted interval. Remarkably, none of the credible sets includes includes areas of the posterior below <span class="math inline">\(0.5\)</span>!</p>
<p>We can also compute the Chen-Shao HPD credible set.</p>
<pre class="r"><code>p = sort(p) 
N.cand = round(N * (1 - alpha)) # number of candidate HPDs
C = matrix(NA, nrow = (N - N.cand), ncol = 2) 
for (i in 1:(N - N.cand)) C[i,1:2] = c(p[i], p[i + N.cand])
diffC = apply(C, 1, &quot;diff&quot;) #lengths of candidate intervals
idx = which.min(diffC)
CI5 = C[idx,] # Chen-Shao HPD 

tbl1 = rbind(CI0, CI5) # variants of CI
tbl1 = cbind(tbl1, apply(tbl1, 1, diff)) # add difference
rownames(tbl1) = c(&quot;Analytic&quot;, &quot;Chen-Shao&quot;)
colnames(tbl1) = c(&quot;lower&quot;, &quot;upper&quot;, &quot;diff&quot;)
tbl1</code></pre>
<pre><code>##               lower     upper       diff
## Analytic  0.5169908 0.5465681 0.02957723
## Chen-Shao 0.5200886 0.5514537 0.03136509</code></pre>
<p>The analytic HPD is at a different location compared to the Chen-Shao one. However, recall that the underlying model is different, therefore, this should not be surprising. It is noteworthy that the Chen-Shao-HPD is larger than the weighted CI! This can occur since both intervals’ properties are asymptotical. In our case it is an indication that the investigated data set exhibits a symmetric posterior, so that HPD and simple credible intervals (almost) coincide:</p>
<pre class="r"><code>require(&quot;latex2exp&quot;)
ggplot() + theme_classic() + geom_histogram(aes(p, ..density..), bins = 100) +
  labs(y = unname(TeX(&quot;$\\hat{\\pi}(p|x)$&quot;)))</code></pre>
<p><img src="case2_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
